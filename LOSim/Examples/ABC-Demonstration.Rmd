---
title: "Individual based modeling: Parametrisation with Approximate Bayesian Computing"
author: "Severin Hauenstein"
date: "June 5, 2016"
output: 
  html_document:
    theme: spacelab
    fig_caption: true
bibliography: ../../thesis/literature/MScAthene.bib
---

```{r, echo = FALSE, eval = TRUE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, warning=FALSE, message=FALSE)
```


```{r, echo=FALSE, eval = TRUE}
library(kfigr)
library(captioner)
figs <- captioner(prefix="Figure")

# load RData if necessary and available
load("ABC-Demonstration.RData")
```




```{r, eval = TRUE}
# set randomisation seed
set.seed(14)

# load packages
library(LOSim)
library(RandomFields)
library(plotrix)
library(randomForest)
```

# Introduction
Statistical models usually provide an analytical approach to the likelihood function, which renders statistical inference readily feasible. Whereas, for complex simulation models an analytical formula is intractable or the likelihood function evaluation might be computationally burdensome.
In these cases approximation can be a solution. The principle in Approximate Bayesian Computation (ABC) is that the intractable likelihood function can be approximated by the mathematical difference between the observed and simulated data, which can be evaluated as the standardised Euclidean distance on a number of summary statistics. 
The final models, i.e. the parameter combinations with the highest approximated likelihoods are frequently chosen by ABC rejection, an approach originally proposed by @Tavare1997. Here, the calculated distances are filtered by a cutoff value $\epsilon$ above which all models are rejected. 

In this short note, I use ABC rejection to parametrise a simple Individual Based Model (IBM), which will eventually be developed for the movement of juvenile little owls (*Athene noctua*) during their dispersal. This will require an increased model complexity. Yet, the current status provides a model infrastructure excellent for testing and presenting the parameterisation approach via ABC.  


# Movement model description
The IBM here is basically a rule-based random walk. That is to say, starting from a current location ($x_t, y_t$) a next location ($x_{t+1}, y_{t+1}$) is selected by drawing from a multinomial distribution with weights for each cell within a perception range $\theta_1$. 
The weights $\omega$ are computed based on the following mathematical function: 

$$\omega(x, y | \theta)_{\eta, \upsilon} =  \frac{e^{- \frac{(E_{\eta, \upsilon} - \theta_2)^2}{\theta_3^2}}}{1 + e^{- \frac{\sqrt{\eta^2 + \upsilon^2} - \theta_5\theta_1} {\theta_4}}},$$

where $\eta$ and $\upsilon$ represent the cells in x- and y-direction within perception range $\theta_1$. The nominator of the major fraction displays the selection term for the implemented environmental variable $E$. It allows the flexibilty of a normal distribution by assigning maximum weights to the the niche optimum $\theta_2$ with a standard deviation, or ecologically speaking a niche range $\theta_3$.
The denominator is basically the logistic distribution function and allows a logistic dependance on the step length with a scale $\theta_4$ and a location parameter $\theta_5$.

Thus, $\eta$ and $\upsilon$ are $selected$ from the specific multionomial distirbution such that 

$$(x_{t+1}, y_{t+1}) = (x_t+\eta_{selected}, y_t+\upsilon_{selected}).$$

On top of the "exact" movement model there is an observation module implemented. So far this is only some Gaussian noise (with a mean of zero and a standard deviation $\sigma$) added to the resulting path, i.e. 
$$(x, y)_{observed} = (x + \mathcal{N}(0, \sigma), y + \mathcal{N}(0, \sigma)).$$ 
Eventually, the observation error is implemented to account for issues of imprecision in the field, which the ecological model by itself could not reflect.

As model output we derive a matrix of $x, y, x_{observed}, y_{observed}$ and $E$ (columns) for all steps including the start location (rows) per each individual.

# Data simulation

The environmental input $E$ are simulated gaussian random fields based on a stationary isotropic covariance model on a 1000 x 1000 cells grid. The values were scaled to [0,1].  

## Environment
```{r, cache = TRUE}
lat <- 1000; lon = 1000 # number of gridcells in latitudinal and longitudinal direction
expCov <- RMexp(var = 0.1, scale = 0.1) # exponential covariance model
env <- RFsimulate(expCov, x = seq(0,1,length.out = lon), # simulate random fields
                                y = seq(0,1,length.out = lat), spConform = FALSE) # using the exp-cov model
# scale to [0;1]
env <- (env - min(env)) / (max(env) - min(env))
```

```{r, eval = TRUE, echo = FALSE, fig.width=6, fig.height=6, fig.align="center"}
# plot environment
dats <- data.frame(expand.grid(seq(lat), seq(lon)), c(env)) # transform to 2 col matrix
colnames(dats) <- c("lat", "lon", "env")
plot(dats$lon, dats$lat, pch=15, cex=1.1, col=terrain.colors(101)[ceiling(dats$env*100)+1], xlab = "Longitude", ylab = "Latitude")
color.legend(xl=lat +60, yb=lon -200, xr=lat + 75, yt=lon, legend=c(0,1), rect.col=terrain.colors(101), gradient="y", align="rb", xpd = NA)
rm(dats)
```


## Observed data
Since I work with simulated data completely, I used the model to simulate the "observed" data with specific parameter values. Imagine the movement data we've got is for 300 individuals, i.e. 300 modeled replicates with fix parameter values.   

```{r, cache = TRUE}
# set parameter values for observation data

startX <- 500
startY <- 500
pRanges <- 8 # perception range (theta 1)
nOpts <- 0.5 # niche optimum (theta 2)
nRanges <- 0.15 # niche range (theta 3)
steplengthScale <- 1000 # steplength scale (theta 4);  > 0 # flat
steplengthLocationFactor <- 0.5 # steplength location (theta 5) # factor of pRanges # 0.5 = 0.5*theta 1
obsErrorReal <- 2.4 # observation model sigma (epsilon)

# 300 replicates
trueParameters <- matrix(rep(c(startX, startY, pRanges, nOpts, nRanges, 
                               steplengthScale, steplengthLocationFactor, obsErrorReal), 300), 
                         nr = 300, byrow = TRUE)


# run observation data model
  
dataTrue <- LOSim::runSimulation(env, iterations = 200, trueParameters, randomSeed = 14)

```

# ABC based parameterisation 

## Parameter sampling
Imagine we have only little clue about the true parameters. Yet, the simulation models I use as reference to the observed data require parameters. These are usually sampled from prior distributions. As indicated the prior knowledge about the parameters is limited, in this case to their approximate ranges. Therefore, I drew 50,000 values from uniform distributions with reasonable ranges for $\theta_2$, $\theta_3$ and $\sigma$. For $\theta_1$ 50,000 integer values were sampled. 
However not for $\theta_4$ and $\theta_5$. These two parameters were fixed to their true values, because the summary statistics to describe them well have yet to be found.  
```{r, cache = TRUE}
## sample parameters
n = 50000
parSample <- cbind(rep(startX, n), # starX
                   rep(startY, n), # startY
                   sample(1:15, size = n, replace = TRUE), # perception range
                   runif(n, 0, 1), # niche optimum
                   runif(n, 0.05, 0.5), # niche range // cannot be 0
                   rep(steplengthScale, n),
                   rep(steplengthLocationFactor, n),
                   runif(n, 0, 5)) # observation error

# parameter names
parameterNames = c("perception range", "niche optimum", "niche range", "observation error")

# run simulation model
data <- LOSim::runSimulation(env, iterations = 200, parSample, randomSeed = 15)

```


## Summary statistics
Selecting a set of informative summary statistics often proves to be difficult, as experienced with $\theta_4$ and $\theta_5$. 
Still, there are several methods to find the best fitting vector of summary statistics for the parameters of interest. The first approach and simplest one is to guess the summary statistic individually, such that there is one summary statistic for each parameter of interest. 
Since the environmental niche selection is implemented as a normal distribution I assumed that mean and standard deviation of the environmental values suit to estimate $\theta_2$ and $\theta_3$, respectively.
Perception range and observation error are rather distance related measures, i.e. the distance between the locations should offer clue to the fit of a the parameters. Since the observation error is implemented as standard deviation in a normal distribution my best guess to fit $\sigma$ is to include the standard deviation of distances between locations as summary statistic. Since the perception range is rather a cumulated distance measure the mean distance over a lag of 10 locations seemed to be a reasonable summary statistic to infer the distribution of $\theta_1$.
```{r, cache = TRUE}
summaryStatistics <- function(dat){
  meandisplacement10 = mean(sqrt(diff(dat$xObs, lag = 10)^2 + diff(dat$yObs, lag = 10)^2), na.rm = T)
  meanEnv <- mean(dat$Environment)
  sdEnv <- sd(dat$Environment)
  sdDisp <- sd(sqrt(diff(dat$xObs)^2 + diff(dat$yObs)^2))
  
  return(c(meandisplacement10 = meandisplacement10, # for pRange
           meanEnv = meanEnv, # for niche optimum
           sdEnv = sdEnv, # for niche range
           sdDisp = sdDisp#, # for observation error
           ))
} 
```

A second approach to derive summary statistics over which to compare the observed with the simulated data is to use predictions from machine learning methods, e.g. RandomForest [@Breiman2001]. Here a larger set of less attributed summary statistics serves as predictor variables [@Pudlo2016, @Marin2016].
```{r, cache = TRUE}
summaryStatistics2 <- function(dat){
  displacement = sqrt(diff(dat$xObs)^2 + diff(dat$yObs)^2)
  meandisplacement1 = mean(displacement)
  meandisplacement2 = mean(sqrt(diff(dat$xObs, lag = 2)^2 + diff(dat$yObs, lag = 2)^2), na.rm = T)
  meandisplacement5 = mean(sqrt(diff(dat$xObs, lag = 5)^2 + diff(dat$yObs, lag = 5)^2), na.rm = T)
  meandisplacement10 = mean(sqrt(diff(dat$xObs, lag = 10)^2 + diff(dat$yObs, lag = 10)^2), na.rm = T)
  sdDisp <- sd(displacement)
  quantilesDisp = quantile(displacement, probs = c(0.25, 0.75))
  
  
  meanEnv <- mean(dat$Environment)
  sdEnv <- sd(dat$Environment)
  quantilesEnv = quantile(dat$Environment, probs = c(0.25, 0.75))
  
  return(c(meandisplacement1 = meandisplacement1, 
           meandisplacement2 = meandisplacement2,
           meandisplacement5 = meandisplacement5,
           meandisplacement10 = meandisplacement10,
           sdDisp = sdDisp,
           quantilesDisp = quantilesDisp,
           meanEnv = meanEnv,
           sdEnv = sdEnv, 
           quantilesEnv = quantilesEnv
           ))
} 

```

For both methods the first step is to compute the summary statistics for the observed and simulated data. 
```{r, cache = TRUE}
summariesTrue <- t(sapply(dataTrue, summaryStatistics))
summaries <- t(sapply(data, summaryStatistics))

summariesTrue2 <- t(sapply(dataTrue, summaryStatistics2))
summaries2 <- t(sapply(data, summaryStatistics2))
```

Then the random forests are built using the parameter sample as response variable. To reduce correlation issues the model predictions are subsequently included in the models for the following parameter samples. The number of trees  necessary to build the models are assessed in fig. `r figr("rfPlot", type="reference")`.
```{r, cache = TRUE}
rfsummaries <- list()
predsSummaries <- as.data.frame(matrix(0, ncol = 4, nrow = NROW(summaries2)))
colnames(predsSummaries) <- c("pS3", "pS4", "pS5", "pS8")
predsSummariesTrue <- as.data.frame(matrix(0, ncol = 4, nrow = NROW(summariesTrue2)))
colnames(predsSummariesTrue) <- c("pS3", "pS4", "pS5", "pS8")
  
index <- 1
for(i in c(3,4,5,8)){
  rFdata <- data.frame(y = parSample[,i], summaries2, predsSummaries)
  rFdataTrue <- data.frame(summariesTrue2, predsSummariesTrue)
  rfsummaries[[index]] <- randomForest(y ~ ., data = rFdata, ntree = 300)
  
  predsSummaries[,index] <- predict(rfsummaries[[index]], newdata = rFdata)
  predsSummariesTrue[index] <- predict(rfsummaries[[index]], newdata = rFdataTrue)  
  
  index <- index + 1
}
```

```{r, rfPlot, eval = TRUE, echo = FALSE, fig.width=12, fig.height=3, fig.align="center"}
par(mfrow=c(1,4))
for(index in 1:4) plot(rfsummaries[[index]], main = "")
```

`r figs(name="rfPlot", "Diagnostics plot for the four RandomForest models, illustrating how quick the error is coming down with an increasing number of trees.")` 

## ABC rejection
The rejection for distance values $> \epsilon$ requires some sort of distance measure between the summary statistics of observed and simulated data. Here I computed the most frequently used Euclidean distance for the standardised summary statistics.  

```{r, chache = TRUE}
  # compute delta / standardise by standard deviation for fixed parameters
  meanSummaryTrue <- colMeans(summariesTrue)
  sdTrue <- apply(summariesTrue, 2, sd)
  distance <- apply(summaries, 1, function(x) sqrt(sum((x - meanSummaryTrue)^2 / sdTrue^2)))
  
  meanSummaryTrue2 <- colMeans(predsSummariesTrue)
  sdTrue2 <- apply(predsSummariesTrue, 2, sd)
  distance2 <- apply(predsSummaries, 1, function(x) sqrt(sum((x - meanSummaryTrue2)^2 / sdTrue2^2)))
```

To decide for a reasonable cutoff value $\epsilon$ we have a look at the histograms of the euclidean distances (see fig. `r figr("distHist", type="reference")`). One approach is to choose $\epsilon$ such that a certain amount of parameter combinations will be accepted. Such as 1000, which is the case for $\epsilon_1 \approx 2.6$ and $\epsilon_2 \approx 3.6$, respectively. Another approach suggested by @Beaumont2002 would be to set  the  percentage  of accepted  simulations, i.e. the  acceptance  rate to  a  specific  value. 

```{r distHist, chache = TRUE, fig.width=12, fig.height=6, echo = FALSE, eval = TRUE, fig.align="center"}
par(mfrow=c(1,2))  
hist(distance, xlab = "distance", ylab = "frequency", main = "")
hist(distance2, xlab = "distance", ylab = "", main = "")
```

`r figs(name="distHist", "Histograms of the Euclidean distances of all sampled parameter combinations. Left panel for guessed summary statistics, right panel for modeled summary statistics using random forest.")` 

```{r}
eps <- 2.6
length(which(distance < eps))

eps2 <- 3.6
length(which(distance2 < eps2))


# cutoff at epsilon
filter <- as.data.frame(parSample[distance < eps, c(3,4,5,8)])
colnames(filter) = parameterNames

filter2 <- as.data.frame(parSample[distance2 < eps2, c(3,4,5,8)])
colnames(filter2) = parameterNames
```


# Results
Since the true parameter values were set to a fix value I would expect to find fairly accentuated distributions for the filtered, i.e. accepted parameter values. This is best assessed by looking at the marginal distributions.
Fig. `r figr("vioPlot", type="reference")` shows the marginal plots for the parameters of interest for both approaches. In the left panel we notice wider distributions for all four parameters. In both right and left panels we find the true parameter values located fairly close to the resepctive mean of the parameter distributions, except for the niche range which seems to be poorly estimated.
In contrast, the two distance related parameters $\theta_1$ and $\sigma$ are estimated remarkably well, and in the case of the modeled summary statistics these two parameter distribution appear highly symmetrical.   

```{r vioPlot, chache = TRUE, fig.width=12, fig.height=6, fig.align = "center", echo = FALSE, eval = TRUE}
# modified BayesianTools::marginalPlot() function
par(oma = c(0,3.5,0,0), xpd = NA, mar = c(2,4,4,0), mfrow = c(1,2))
library(vioplot)
plot(NULL, ylim = c(0, NCOL(filter) + 1), type = "n", yaxt = "n", 
        xlab = "", ylab = "", xlim = range(filter), main = "")
for (i in 1:NCOL(filter)) {
    vioplot::vioplot(filter[, i], at = i, add = T, col = "darkred", 
        horizontal = T)
    axis(side = 2, at = i, labels = parameterNames[i], las = 1)
}
points(c(pRanges, nOpts, nRanges, obsErrorReal), 1:NCOL(filter), cex = 3, pch = 4, lwd = 2)

plot(NULL, ylim = c(0, NCOL(filter2) + 1), type = "n", yaxt = "n", 
        xlab = "", ylab = "", xlim = range(filter2), main = "")
for (i in 1:NCOL(filter2)) {
    vioplot::vioplot(filter2[, i], at = i, add = T, col = "darkred", 
        horizontal = T)
    #axis(side = 2, at = i, labels = parameterNames[i], las = 1)
}
points(c(pRanges, nOpts, nRanges, obsErrorReal), 1:NCOL(filter), cex = 3, pch = 4, lwd = 2)
```

`r figs(name="vioPlot", "Violin plot illustrating the filtered paramter distributions. True parameters, i.e. the values used for the simulation of the observed 300 individuals are marked as crosses over the respective violine. Left panel for guessed summary statistics, right panel for modeled summary statistics using random forest.")` 

We also look at the correlation structure within the accepted parameter sample. Fig. `r figr("CP", type="reference")`  and fig. `r figr("corPlot2", type="reference")` illustrate well the clustering of parameter values, which ideally resembles the correlation structure of the true parameter values. Since in this case those were fixed to specific values the clustering would ideally arise tightly around the true values pairs. We find a more suitable structure in fig. `r figr("corPlot2", type="reference")`. Instead, Fig. `r figr("CP", type="reference")` not only shows wider ranges (as in fig. `r figr("vioPlot", type="reference")`, left panel) but also less clustering around true parameter pairs.    
```{r CP, chache = TRUE, fig.width=6, fig.height=6, fig.align = "center", echo = FALSE, eval = TRUE}
BayesianTools::correlationPlot(filter, density = "smooth")
```

`r figs(name="CP", "Correlation plots for the filtered paramter values with parameter histograms on the diagonal, pearson's r on the upper right and correlation density plots on the lower left.")` 

```{r corPlot2, chache = TRUE, fig.width=6, fig.height=6, fig.align = "center", echo = FALSE, eval = TRUE}
BayesianTools::correlationPlot(filter2, density = "smooth")
```

`r figs(name="corPlot2", "Correlation plots for the filtered paramter values with parameter histograms on the diagonal, pearson's r on the upper right and correlation density plots on the lower left.")` 


# Discussion

Meaningful modelling of ecological processes requires an adequate model structure as well as parameterising the model with appropriate values. Both can be tricky. The former is highly dependant on the knowledge about the system to be modelled. The latter certainly also depends on the model structure, hence on how well the model reflects the signal in the data. Furthermore finding the "correct" parameter values relies on an adequate parameterisation procedure. 

In this short note I used ABC rejection to parameterise a simple IBM. In addition to selecting the summary statistcs individually and by best guess, I used RandomForest models based on a wider range of reasonably selected summary statistics. With these I predicted measures on which the Euclidean distance between observed and simulated data was calculated. 

The parameters related to distance measures, perception range and observation error, were fitted promisingly. Both posterior parameter distributions are narrow and with a maximum virtually at the respective true value. However, since the model to parameterise was fairly simple the expectations to find all the true parameter values were quite high. Yet, the parameters influencing the envionmental niche selection caused problems, specifically refering to the niche range. 

There are two quite obvious solutions to this: (a) One aims to find summary statistcs that better reflect the outcome of the model output with changing parameter values or (b) one removes the problematic parameters from the fitting procedure. The latter option however requires gathering more prior knowledge on the processes the respective parameters are a part of. This seems particularly feasable for environmental niche selection processes. The wide toolbox of statistical habitat selection analysis can provide solutions here.   

With increasing model complexity the number of parameters to fit usually increases as well. This is what @Blum2010a refer to as "curse of dimensionality". In consequence, the parameterisation becomes more and more difficult and the performance of ABC progressively relies on sophisticated execution. 

## Tuning the summary statistics
If one chooses to tune the model choice by twiddling with the summary statics there are various methods recently discussed in the scientific literature. Instead of fitting RandomForest models to the summary statistics other machine learning approaches are suggested to infer robust distance measures, such as Artificial Neural Networks [@Blum2010a]. The semi-automatic ABC uses an additional estimation stage of the optimal summary statistics, which is then used to estimate the summary statistics in the ABC. This is a method originally proposed by @Fearnhead2012 and described as robust way tho choose the summary statistics. @Fearnhead2012 also suggested to employ a partial least square regression to linearly model the summary statistics. Potential drawbacks of this approach are that the relationship of the summary statistics and the parameter sample is non-linear [see also @Wegmann2009]. Another option to select the "best" summary statistics was proposed by @Joyce2008 who assigned scores to the statistics based on whether they substantially contributed to an impovement of the inference process. 

Apparently an increasing model complexity requires an increasing complexity in the ABC. However, in addition to higher technical requirements most of the above described methods implicate an increased computational effort. A perhaps less costly alternative to tuning the summary statistics is post sampling regression adjustment, originally proposed by @Beaumont2002. This is a local-linear regression where the accepted parameter values are adjusted to a linear transform [@Csillery2010, @Blum2010]. 

Coming from a different perspective, meaning not ABC rejection, Markov Chain Monte Carlo (MCMC) algorithms are efficient methods that explore the parameter space iteratively. The distance between simulated and observed summary statistics are updated to current parameter values. Thus parameter values resulting in simulations close to the observed data are visited preferably [@Marjoram2003, @Wegmann2009]. 

In conclusion, we find us confronted with a range of variations within the ABC framework. All methods and approches have been developed within the last 30 years and offer solutions to specific parameterisation problems and yet it seems that there is no *ne plus ultra* for all of them. Rather than that there are adaptations to the overall method, ABC, to address many of the difficulties, such as computational costs or accuracy.  


# References














```{r, eval = FALSE, echo = FALSE}
plotResult <- function(res, par){
  par(mfrow = c(2,3))
  plot(res$parameters[,par], res$distance, col = 1, xlab = paste("parameter", par), ylab = "distance")
  
  for (i in 1:ncol(res$summaries)){
      plot(res$parameters[,par], res$summaries[,i] - res$summariesTrue[2,i], 
           col = 1, xlab = colnames(res$summaries)[i])
    abline(h=0, col = "red")
    
  }
  par(mfrow = c(1,1))
}

plotResult(res, 8)



library(BayesianTools)

eps <- 7
length(which(res$distance < eps))

filter <- as.data.frame(res$parameters[res$distance < eps, c(3,4,5,8)])
colnames(filter) = res$parameterNames

marginalPlot(filter, best = c(pRanges, nOpts, nRanges, obsErrorReal))
correlationPlot(filter, density = "smooth")


```



```{r, eval = FALSE, echo = FALSE}
# # check summary statistics by guess
# windows(10,10)
# betterPairs(data.frame(meanEnv = res$summaries[,1], 
#                        varEnv = res$summaries[,2], 
#                        meandisplacement = res$summaries[,3], 
#                        meandisplacement5 = res$summaries[,4],
#                        nicheOpt = res$parameters[,4],
#                        nicheRange = res$parameters[,5],
#                        perceptionRange = res$parameters[,3],
#                        Obserror = res$parameters[,6]))
# # cor(simSummary.s)

```


```{r, eval = FALSE, echo = FALSE}
plotPosterior <- function(res, par.pair = NULL, epsilon = NULL, 
                          eps.color = c("lightgrey", "darkgrey", "red"),
                          eps.pch = c(1, 18, 8),
                          main = "Accepted parameters for \n different values of epsilon",
                          legend = TRUE, legend.position = "topleft",
                          show.truth = TRUE, true.par.1 = NULL, true.par.2 = NULL){
  if(is.null(par.pair)) stop("Choose pair of parameters to plot against each other.")
  if(is.null(epsilon)) stop("Choose epsilon values as rejection thresholds. Must be vector of three.")
  
  plot(res$parameters[res$distance < epsilon[1], par.pair], col = eps.color[1], pch = eps.pch[1],
       xlab = paste("parameter", par.pair[1]), ylab = paste("parameter", par.pair[2]), main = main)
  points(res$parameters[res$distance < epsilon[2], par.pair], pch = eps.pch[2], col = eps.color[2])
  points(res$parameters[res$distance < epsilon[3], par.pair], pch = eps.pch[3], col = eps.color[3])
  
  if(legend) legend(legend.position, paste("<", epsilon), pch = eps.pch, col = eps.color, bty = "n")
  
  if(show.truth){
    abline(v = true.par.1)
    abline(h = true.par.2) 
  }
  
}

plotPosterior(res, c(3,7), epsilon = c(3,2,1), true.par.1 = pRanges, true.par.2 = obsErrorReal)

library()

```

